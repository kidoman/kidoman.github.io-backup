<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: node.js | Karan Misra]]></title>
  <link href="http://kidoman.io/tags/node-dot-js/atom.xml" rel="self"/>
  <link href="http://kidoman.io/"/>
  <updated>2014-05-15T15:18:34+05:30</updated>
  <id>http://kidoman.io/</id>
  <author>
    <name><![CDATA[Karan Misra]]></name>
    <email><![CDATA[karan.misra@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Async IO - Part 1]]></title>
    <link href="http://kidoman.io/programming/async-io-part-1.html"/>
    <updated>2014-05-13T16:24:00+05:30</updated>
    <id>http://kidoman.io/programming/async-io-part-1</id>
    <content type="html"><![CDATA[<p>I was recently reading a <a href="http://venkateshcm.com/2014/04/Reactor-Pattern-Part-4-Write-Sequential-Non-Blocking-IO-Code-With-Fibers-In-NodeJS/">series</a> on &ldquo;Write Sequential Non-Blocking IO Code With Fibers in NodeJS&rdquo; by <a href="http://venkateshcm.com/">Venkatesh</a>.</p>

<p>Venki was essentially trying to emphasize that writing non-blocking code in NodeJS (either via callbacks, or using promises) can get hairy really fast. For example, this code demonstrates that aptly:</p>

<p>```javascript Callback driven NodeJS
var express = require(&lsquo;express&rsquo;);
var app = express();</p>

<p>app.get(&lsquo;/users/:fbId&rsquo;, function(req, res) {
  var id = req.params.id;
  var key = &lsquo;user:&rsquo; + id;
  client.get(key, function(err, reply) {</p>

<pre><code>if (err !== null) {
  res.send(500);
  return;
}

if (reply === null) {
  res.send(404);
  return;
}

res.send(200, {id: id, name: reply});
</code></pre>

<p>  });
});</p>

<p>```</p>

<p>The exact code is available on <a href="https://github.com/kidoman/fibrous/blob/master/nodejs/callback.js#L59-L72">GitHub</a> (so is the <a href="https://github.com/kidoman/fibrous/blob/master/nodejs/promise.js#L55-L65">promises driven version</a>, but I won&rsquo;t bother inlining it.)</p>

<p>What we actually wanted to write (if it were possible, was):</p>

<p>```javascript Not plain old JavaScript
var express = require(&lsquo;express&rsquo;);
var app = express();</p>

<p>app.get(&lsquo;/users/:fbId&rsquo;, function(req, res) {
  var id = req.params.id;
  var key = &lsquo;user:&rsquo; + id;</p>

<p>  try {</p>

<pre><code>var reply = client.get(key);
if (reply === null) {
  res.send(404);
  return;
}

res.send(200, {id: id, name: reply});
</code></pre>

<p>  }
  catch(err) {</p>

<pre><code>  res.send(500);
</code></pre>

<p>  }
});
```</p>

<p>The magic would happen in line number 9 (above.) Instead of having to provide a cascade of callbacks (what if we wanted to do another lookup after we got the value back from the first), we could just write them serially, one after the other.</p>

<p>Well. Apparently we can!</p>

<h2>Fibers</h2>

<blockquote><p>A fiber is a particularly lightweight thread of execution. Like threads, fibers share address space. However, fibers use co-operative multitasking while threads use pre-emptive multitasking. Threads often depend on the kernel&rsquo;s thread scheduler to preempt a busy thread and resume another thread; fibers yield themselves to run another fiber while executing.</p></blockquote>

<p>Fibers allow exactly this kind of black magic in NodeJS. It is still callbacks internally, but we are exposed to none of it in our application code. Sure you will end up writing a bunch of wrappers (or have some tool generate them for us), but we would have the sweet sweet pleasure of writing async IO code without having to jump through all the hoops. This is how the wrapper code for redis client looks like:</p>

<p>```javascript
var Fiber = require(&lsquo;fibers&rsquo;);
var client = require(&lsquo;./redis-client&rsquo;);</p>

<p>exports.get = function(key) {
  var err, reply;
  var fiber = Fiber.current;
  client.get(key, function(<em>err, </em>reply) {</p>

<pre><code>err = _err;
reply = _reply;
fiber.run();
</code></pre>

<p>  });
  Fiber.yield();
  if (err != null) {</p>

<pre><code>throw err;
</code></pre>

<p>  }
  return reply;
};
```</p>

<p>(the <a href="https://github.com/kidoman/fibrous/blob/master/nodejs/fiber.js#L52-L60">real code</a> is here in case you are curious)</p>

<p>I liked how the code looked. Having survided a &lsquo;promising&rsquo; node.js project, I was definitely curious about this new style. Maybe this can be the saving grace (before generators and <strong>yield</strong> take over the JS world) for real world server side JavaScript.</p>

<h2>Fibers you say</h2>

<p>But the code (and the underlying technique which makes it tick) sounded very familiar, and reminded me of a similar technique which is used in Go to allow writing beautiful async IO code. For example, the same function from above in Go:</p>

<p>```go
m.Get(&ldquo;/users/:id&rdquo;, func(db *DB, params martini.Params) (int, []byte) {
  str := params[&ldquo;id&rdquo;]
  id, err := strconv.Atoi(str)
  if err != nil {</p>

<pre><code>return http.StatusBadRequest, []byte{}
</code></pre>

<p>  }</p>

<p>  u, err := db.LoadUser(id)
  if err != nil {</p>

<pre><code>return http.StatusNotFound, []byte{}
</code></pre>

<p>  }
  return http.StatusOK, encoder.Must(enc.Encode(u))
})
```</p>

<p>Sure, there is a little more happening in here (Go is statically typed), buts its the exact same thing as the fibers example, without all the manual wrapping. Any call which does IO (like line 8) blocks the currently executing goroutine (just like a fiber, a lightweight thread.) The natural question to ask is, if the goroutine gets blocked, how do other requests get processed? Its quite simple actually. The Go runtime automatically schedules any other goroutine which is ready to run (their IO call is done) on the thread on which the current goroutine was running.</p>

<p>Since goroutines are light weight (stack size is just 4 KB in Go 1.3beta1 compared to the much larger ~2 MB thread stacks), it is not unusual to have hundreds of thousands of goroutines actively running in a single process, all humming along together. The best part, since the threads have to do less context switching (the same physical thread can continue running on the processor core, just the instruction pointer keeps changing as the goroutines shuffle in and out, just as in method calls), we are able to extract a lot more efficiency from the same unit of hardware than otherwise. Otherwise IO calls, which would otherwise cause the thread to block and wait, could cripple the system and bring it down to its knees. Read <a href="http://venkateshcm.com/2014/05/How-To-Determine-Web-Applications-Thread-Poll-Size/">this</a> article for more context on this.</p>

<h2>Performance</h2>

<p>A fellow ThoughtWorker asked me, &ldquo;Does performance matter when choosing a framework?&rdquo;</p>

<p>I know where he was coming from, and how we shouldn&rsquo;t make decisions purely based on performance (we would all be doing assembly if that was the case.) While it is true that as a startup (or even in the case of a well established player), building the MVP and getting it to the users is paramount, you really dont want to face the situation where you suddenly have a huge influx of users (say it goes viral) and you are caught between a ROCK (scale horizontally by throwing compute units at the problem) and a HARD PLACE (have to rewrite the solution in a technology more amenable to scaling.) Both of these options are expensive, and can potentially be a deal breaker.</p>

<p>Therefore, provided everything else is more or less equal, choosing the more performant one is never a bad thing.</p>

<p>With this context, I decided to compare the two solutions for their performance, given that they more or less looked the same. I decided to allow the system under test to use as many cores as they wanted, and then hit them with 100 concurrent users, each of which is going full tilk for around 20 seconds (used the awesome <a href="https://github.com/wg/wrk">wrk</a> tool for benchmarking.)</p>

<p>The results:</p>

<table>
<thead>
<tr>
<th>Golang  </th>
<th> &nbsp;</th>
</tr>
</thead>
<tbody>
<tr>
<td>Stdlib  </td>
<td> <a href="https://github.com/kidoman/fibrous/blob/master/go/stdlib.go">134566</a> (3.81ms)</td>
</tr>
<tr>
<td>Martini </td>
<td> <a href="https://github.com/kidoman/fibrous/blob/master/go/martini.go">51330</a> (9.51ms)</td>
</tr>
</tbody>
</table>


<table>
<thead>
<tr>
<th>node.js   </th>
<th> &nbsp;</th>
</tr>
</thead>
<tbody>
<tr>
<td>Stdlib    </td>
<td><a href="https://github.com/kidoman/fibrous/blob/master/nodejs/stdlib.js">54510</a> (7.78ms)</td>
</tr>
<tr>
<td>Callbacks*</td>
<td><a href="https://github.com/kidoman/fibrous/blob/master/nodejs/callback.js">36107</a> (10.84ms)</td>
</tr>
<tr>
<td>Fibers*   </td>
<td><a href="https://github.com/kidoman/fibrous/blob/master/nodejs/fiber.js">27372</a> (18.76ms)</td>
</tr>
<tr>
<td>Promises* </td>
<td><a href="https://github.com/kidoman/fibrous/blob/master/nodejs/promise.js">22665</a> (17.15ms)</td>
</tr>
</tbody>
</table>


<p>* The Callbacks, Fibers and Promises versions are created using Express. The Stdlib versions use the <strong>http</strong> support in the corresponding standard libraries.</p>

<p>All the numbers are in <strong>req/s</strong> as given by wrk (higher is better.) The latency details are in brackets (lower is better.) Clicking the numbers will take you to the corresponding code in the <a href="https://github.com/kidoman/fibrous">GitHub repo</a> (the <a href="https://github.com/kidoman/fibrous/blob/master/README.md">README</a> has the detailed numbers.)</p>

<p>The tests were done on an updated Ubuntu 14.04 box with a Intel i7 4770 processor, 16 GB of RAM and a SSD.</p>

<p>As you can see, the <strong>fibers</strong> method of doing async IO in <strong>node.js</strong> comes with a perceivable loss in throughput compared to the pure <strong>callbacks</strong> based approach, but looks relatively better than the <strong>promises</strong> version for this micro-benchmark.</p>

<p>At the same time, the default way of doing IO in Golang does very well for itself. More than <strong>134,000 req/s</strong> with a <strong>3.81 ms</strong> 99th percentile latency. All this without having to go through crazy callbacks/promises hoops. How cool is that?</p>

<h2>How the tests were run?</h2>

<h3>Software versions</h3>

<ul>
<li>Go 1.3beta1</li>
<li>node.js 0.10.28</li>
<li>wrk 3.1.0</li>
</ul>


<h3>Command used to run</h3>

<p>A more detailed description is available in the <a href="https://github.com/kidoman/fibrous">README</a> but I will explain a simple version here:</p>

<ul>
<li>Start the program (by say running ./start_martini.sh)</li>
<li>Run the benchmark (by running ./bench.sh)</li>
<li>Record the result</li>
<li>Rince and repeat 3 times and take the best run</li>
</ul>


<h3>Notes</h3>

<ul>
<li>All cores on the Intel i7 4770 were set to the performance governor</li>
<li>Redis was not tweaked</li>
<li>ulimit was not raised</li>
</ul>


<h2>Summary</h2>

<p>This is part 1 in a multipart series looking at how async IO (and programming in general) is done in various languages/platforms. We will be going indepth into one language/platform with the every new article in the series. Future parts will look at Scala, Clojure, Java, C#, Python and Ruby based frameworks and try and present a holistic view of the async world.</p>

<p>But one thing is very clear, async IO is here to stay. Not embrassing it would be foolhardy given the need to stay lean. Hope these articles help you understand gravity of the decision.</p>

<p>While some might argue that what we did in Golang was not really async, as the call was blocking in nature. But the net result achieved, and the reason why Go is still able to provide an awesome throughput despite blocking IO calls, is because the Go runtime essentially does the heavy lifting for you. When one goroutine is busy waiting for the results of a IO call to come back, other goroutines can take their place and not waste CPU cycles. The fact that this mechanism allows us to get away with fewer threads that would be required otherwise, is the icing on top.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[node.js in action: Learnings]]></title>
    <link href="http://kidoman.io/engineering/node-dot-js-in-action-learnings.html"/>
    <updated>2013-05-31T01:58:00+05:30</updated>
    <id>http://kidoman.io/engineering/node-dot-js-in-action-learnings</id>
    <content type="html"><![CDATA[<p><img src="/images/node.js.png" title="nodejs" ></p>

<p>I recently gave a talk on node.js at work&hellip; the talk was surprisingly well attended. It was scheduled to start at 1 PM. At around 1:02 PM, I was alone in the room with not another soul in sight. The room filled to the brim in the next 10 minutes. Last count = 33</p>

<h1>Link to the talk</h1>

<p><a href="http://goo.gl/Pcz2wQ">click</a></p>

<h1>The Talk</h1>

<p>The theme of the talk was to get people excited about the node.js work we were doing for a large enterprise client. Almost 9 months into development, we had figured out a tonne of patterns and idioms and had actually started becoming productive with it. Talking about the hurdles faced during the non avoidable learning curve seemed like a sensible thing to do.</p>

<h2>Tech Stack</h2>

<p>Although I cannot reveal any actual URLs atm, the stack of the application we are developing is as follows:</p>

<ul>
<li>node.js (Platform)</li>
<li>Express (Web Server)</li>
<li>Sequelize (ORM)</li>
<li>MySQL (Database)</li>
<li>Q (Promises)</li>
<li>Mocha/Sinon/Chai (and their -as-promised bretheren, for TDD)</li>
<li>Grunt (Command runner)</li>
<li>Coffee-Resque (For background jobs)</li>
<li>Socket.io (For realtime tracking of connected clients)</li>
<li>Redis (As a datastore for our generated socket.io handshake tokens)</li>
<li>connect-assetmanager (For managing our assets; <strong>update</strong> we have now replaced this with our own hand rolled solution)</li>
</ul>


<h2>Promises to Keep: Q</h2>

<p>I think it is essential for any new node.js project to base itself on a solid promises library. As indicated above, the library of choice for us was Q. I would go out on a limb and say this &ndash; &ldquo;Avoiding callback hell is probably the least interesting feature of Q.&rdquo;</p>

<p>Q allows us to beautifully structure our code without excessive &ldquo;pyramiding.&rdquo; You still end up with a few nested promises from time to time, but that is also avoidable with judicious usage of Q.spread.</p>

<h2>Testing</h2>

<p>Equally important is the need to get the TDD/BDD pattern flowing right from get go. Mocha/Sinon/Chai allow BDD in JavaScript to look almost as elegant as RSpec (which I consider to be the holy grail of developer friendly BDD.) Being smart and using the -as-promised utility node modules will also save you a lot of grief and restore a sense of sanity to the test cases (you can then essentially start returning promises of future asserts from your test cases, instead of having to call a <strong>done()</strong> at the end to signal the end.)</p>

<h2>Scaling and Deployment</h2>

<p>Our dreams of serving a million requests from a single node.js process were quickly shattered when we discovered a pegged 100% CPU (on a single core.) Besides trying to fix the main issue (which turned out to be the MySQL driver being used by sequelize) we also spent some time getting the <a href="http://unicorn.bogomips.org/">unicornification</a> of node.js right. We ended up with a master-slave arrangement which allowed us to scale our application to N-1 cores (where N is the total number of cores available in the machine.) Common sense dictated the <strong>-1</strong> part (to leave a core aside for the kernel and various sub systems.)</p>

<p>Figuring our deployments was also fun. Although there was some talk of building and deploying a .rpm (our target Linux distribution was CentOS 6.4), the general lack of time lead us to adapt a git based deployment mechanism. Any SHA is a good candidate for deployment to any of our environments. We obviously ended up tagging the good ones.</p>

<h2>Conclusion</h2>

<p>We really went out on a limb here by using node.js for the project when we did not actually have much in house expertise on the same. However, the effort has redeemed itself many fold. A lot of people have successfully managed to <strong>get</strong> &ldquo;concurrent programming&rdquo; as a result. Coming out from the comfort zone of RoR, .NET and Java also possibly has changed the course of their lives for ever! :)</p>
]]></content>
  </entry>
  
</feed>
